# Do not touch:

defaults:  
  - _self_  
  - override hydra/hydra_logging: disabled  
  - override hydra/job_logging: disabled  
  
hydra:  
  output_subdir: null  
  run:  
    dir: .

convert_hf:
  in_path: "temp/${model.name}"
  out_path: "temp/hf_${model.name}"

# Touch anything below here instead:

wandb:
  # Name your model
  model_name: "${data.subset}_-${now:%Y-%m-%d_%H-%M-%S}_${data.sampling.num_train}_256bs" 
  # And your project
  proj_name: "ability_transfer"
  num_examples_reported: 100

data:
  format: B
  tok_format: t_search_post_merged
  # length: 25600
  subset: t_search_post/${data.format}
  datapath: data
  train_file: "data/${data.subset}/train.json"
  test_files: ["data/${data.subset}/test.json"]
  # test_files: ["data/${data.subset}/test_0.json", "data/${data.subset}/test_1.json"]
  num_workers: 0

  # Tokenizer - No need to change.
  tokenizer_path: "tokenizer/tokenizer_${data.tok_format}.json"

  # No need to change.
  split_str: "Command:"

  # If you want to subsample sets:
  sampling:
    # Set to True if you want to subsample your sets.
    sample_train_set: True
    sample_test_set: True
    sample_val_set: True

    # This only applies if the above vars are set to True, otherwise uses all the data!!!
    num_train: 1024
    num_val: 1024
    num_test: 1024

  # If you want to filter data (use utils/filter_data.py), otherwise unused.
  filter:
    max_token_length: None  # Maximum token length for filtering examples
  
model:
  name: ${wandb.model_name}

  batch_size: 128
  accumulate_grad_batches: 1
  block_size: 256
  epochs: 1000

  n_layer: 12
  n_head: 8
  n_embd: 128

optim:
  lr_type: linear-reg # options: linear or linear-reg, 
                    # otherwise cosine decay (lr_type: None)
  lr: 2e-4         # learning rate (used for cosine schedule)
  warmup_steps: 10
  # n_steps: ${model.epochs}
  weight_decay: 0.01
  max_lr: 0.002


eval:
  num_examples: 512
  batch_size: 512
  results_dir: "data/eval_results/${model.name}"

inference:
  modelpath: "temp/hf_${model.name}" # Default: uses checkpoint of model.name
  datapath: ${data.datapath}/inference_data/ # Reads ALL json files from this directory
  batch_size: 512

  # If you want to subsample test set for inference:
  sampling:
    sample_test_set: False
    num_test: 512